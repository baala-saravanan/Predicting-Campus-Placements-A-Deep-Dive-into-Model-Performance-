# Logistic Regression, Decision Tree, and K-Nearest Neighbors (KNN) Models
Recently, I took on the task of predicting student placement outcomes using logistic regression, decision tree, and K-nearest neighbors (KNN) models. This project explored how academic performance, extracurriculars, and skill certifications impact students‚Äô placement probabilities. Here‚Äôs what I found:

Logistic Regression stood out with 79.45% accuracy üèÜ, showcasing its robustness for binary classification problems where interpretability and computational efficiency matter. This model offers not only class predictions but also probability scores, providing a well-rounded view of prediction confidence.

K-Nearest Neighbors (K=19) achieved a close second with 79.15% accuracy. Despite its power to capture complex patterns, KNN‚Äôs performance can vary based on the choice of K, making it sensitive to parameter tuning.

Decision Tree at optimal depth (6) attained 78.25% accuracy. Its simplicity and interpretability are appealing, yet it‚Äôs prone to overfitting on this dataset, indicating the need for fine-tuning.

üîç Insights:

Logistic Regression was the top performer due to its balance of accuracy, simplicity, and interpretability.

KNN and Decision Tree offered competitive results, each with unique strengths and areas for application.

This project reinforced the importance of model selection tailored to data characteristics and application needs. Selecting the right model isn‚Äôt just about accuracy ‚Äì it‚Äôs about understanding each model's strengths and ensuring they align with the project goals.

#MachineLearning #DataScience #CampusPlacements #PredictiveModeling #LogisticRegression #KNN #DecisionTree
